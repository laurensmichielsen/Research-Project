# Research-Project
This repository is part of the [Research Project](https://github.com/TU-Delft-CSE/Research-Project) of Academic year 2023-2024 at [Delft University of Technology](https://https//github.com/TU-Delft-CSE).
## Abstract
The annotation effort associated with object detection is extremely costly. One option to reduce cost is to relax the demands on annotation quality, effectively allowing annotation noise. Current research primarily focuses on noise correction before or during training. However, there remains a gap in the research regarding the impact of specific types of human annotation noise on object-detector performance. This research aimed to determine how sensitive object detectors are to human annotation noise. A systematic methodology was developed to generate and quantify the effects of four noise types: missing annotations, extra annotations, inaccurate bounding boxes, and wrong classification labels. Additionally, evaluations were conducted on YOLOv8 and Faster R-CNN using the PASCAL VOC 2012, VisDrone, and Brain-Tumor datasets. The experiments demonstrated that adding noise to smaller datasets adversely affects the performance of object detectors trained on these datasets more than it does for those trained on larger datasets. Similarly, annotation noise in small objects affects detector performance more than large objects. Furthermore, YOLOv8 is resilient to low levels of missing annotations and inaccurate bounding boxes but is sensitive to all levels of incorrect classification labels. Interestingly, extra annotations seem to have a regularization effect on YOLOv8. In contrast, Faster R-CNN is generally more susceptible to annotation noise compared to YOLOv8, particularly concerning extra annotations, though both models display similar trends regarding inaccurate bounding boxes.

## Experiments
The experiments were conducted on three datasets: PASCAL VOC 2012, VisDrone, and Brain-Tumor. The annotation noise was generated by introducing four types of noise: missing annotations, extra annotations, inaccurate bounding boxes, and wrong classification labels. The experiments were conducted on two object detectors: YOLOv8 and Faster R-CNN. The experiments were conducted on both small and large objects to determine the impact of object size on detector performance. The experiments were conducted on both small and large datasets to determine the impact of dataset size on detector performance.

## Reproducing the Experiments
To reproduce the experiments, follow the steps below:
1. Clone the repository
```
git clone https://github.com/laurensmichielsen/Research-Project.git
``` 
2. Install the required packages by running the following command:
```
pip install -r requirements.txt
```
3. Download the datasets from the following links:
    - [PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)
    - [VisDrone](https://github.com/VisDrone/VisDrone-Dataset)
    - [Brain-Tumor](https://docs.ultralytics.com/datasets/detect/brain-tumor/)

3. Create a folder named `datasets` in the root directory of the repository and make three subfolders named `PASCAL_VOC_2012`, `VisDrone`, and `Brain_Tumor`. Ensure that the annotations are in YOLO format since these are required for the noise generation scripts

4. For each dataset, create the following structure
```
dataset
│
└───AllImages
|___AllLabels
|___images
|    |___train
|    |___val
|    |___test
|___labels
|    |___train
|     |___val
|     |___test
|___noise1
|   |___images
|   |___labels
|___noise
```
5. Ensure you have to correct data split for the Experiment you wish to reproduce. The `data-splits` folder contains the splits used in the experiments. The splits are in txt format and contain the image names of the train, validation, and test sets without the image suffix. The Faster R-CNN, VisDrone, and Brain-Tumor experiments all used the same subset and therefore there is only one file in their respective subfolders

    a. You can also create custom splits by collecting all the images and labels and using the split.py file in the main folder. This will create a train, validation, and test split in the datasets.

6. Run the experiment script of the architecture you want to use from the `experiments` folder. Update imports and file paths as needed. The script will run the experiments and save the results in the results folder.